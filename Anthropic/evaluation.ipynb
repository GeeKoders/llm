{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1617c065",
   "metadata": {},
   "source": [
    "## Three Paths After Writing a Prompt\n",
    "\n",
    "Option 1: Test the prompt once and decide it's good enough. This carries a significant risk of breaking in production when users provide unexpected inputs.\n",
    "\n",
    "Option 2: Test the prompt a few times and tweak it to handle a corner case or two. While better than option 1, users will often provide very unexpected outputs that you haven't considered.\n",
    "\n",
    "Option 3: Run the prompt through an evaluation pipeline to score it, then iterate on the prompt based on objective metrics. This approach requires more work and cost, but gives you much more confidence in your prompt's reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4407fb2c",
   "metadata": {},
   "source": [
    "## The Workflow of Prompt Evaluation\n",
    "\n",
    "Step 1: Draft a Prompt\n",
    "\n",
    "Step 2: Create an Eval Dataset\n",
    "\n",
    "Step 3: Feed Through Claude\n",
    "\n",
    "Step 4: Feed Through a Grader\n",
    "\n",
    "Step 5: Change Prompt and Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd566966",
   "metadata": {},
   "source": [
    "## Types of Graders\n",
    "\n",
    "### Code Graders\n",
    "Code graders let you implement any programmatic check you can imagine. Common uses include:\n",
    "\n",
    "* Checking output length\n",
    "* Verifying output does/doesn't have certain words\n",
    "* Syntax validation for JSON, Python, or regex\n",
    "* Readability scores\n",
    "\n",
    "### Model Graders\n",
    "Model graders feed your original output into another API call for evaluation. This approach offers tremendous flexibility for assessing:\n",
    "\n",
    "* Response quality\n",
    "* Quality of instruction following\n",
    "* Completeness\n",
    "* Helpfulness\n",
    "* Safety\n",
    "\n",
    "### Human Graders\n",
    "Human graders provide the most flexibility but are time-consuming and tedious. They're useful for evaluating:\n",
    "\n",
    "* General response quality\n",
    "* Comprehensiveness\n",
    "* Depth\n",
    "* Conciseness\n",
    "* Relevance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998e0b29",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
