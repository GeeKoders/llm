{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1617c065",
   "metadata": {},
   "source": [
    "## Three Paths After Writing a Prompt\n",
    "\n",
    "Option 1: Test the prompt once and decide it's good enough. This carries a significant risk of breaking in production when users provide unexpected inputs.\n",
    "\n",
    "Option 2: Test the prompt a few times and tweak it to handle a corner case or two. While better than option 1, users will often provide very unexpected outputs that you haven't considered.\n",
    "\n",
    "Option 3: Run the prompt through an evaluation pipeline to score it, then iterate on the prompt based on objective metrics. This approach requires more work and cost, but gives you much more confidence in your prompt's reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4407fb2c",
   "metadata": {},
   "source": [
    "## The Workflow of Prompt Evaluation\n",
    "\n",
    "Step 1: Draft a Prompt\n",
    "\n",
    "Step 2: Create an Eval Dataset\n",
    "\n",
    "Step 3: Feed Through Claude\n",
    "\n",
    "Step 4: Feed Through a Grader\n",
    "\n",
    "Step 5: Change Prompt and Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd566966",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
